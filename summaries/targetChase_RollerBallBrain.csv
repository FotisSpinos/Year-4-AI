Steps,Policy/Entropy,Policy/Learning Rate,Environment/Cumulative Reward,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward
10000,1.4189383,0.00029704368,0.2270168855534709,17.80112570356473,0.16639899,0.2274436090225564
20000,1.4189427,0.0002910099,0.2712765957446808,16.72340425531915,0.19464093,0.2712765957446808
30000,1.4188523,0.00028500575,0.3223787167449139,14.613458528951487,0.16251037,0.3223787167449139
40000,1.4184428,0.00027898132,0.38857142857142857,13.291428571428572,0.19914982,0.38857142857142857
50000,1.4176613,0.00027298782,0.46403385049365303,13.091678420310297,0.26037264,0.46403385049365303
60000,1.4154766,0.0002669997,0.5288831835686778,11.843388960205392,0.30590153,0.5288831835686778
70000,1.4133229,0.0002610002,0.5916442048517521,12.46900269541779,0.3772121,0.5916442048517521
80000,1.4108627,0.00025500014,0.6644385026737968,12.368983957219251,0.46242303,0.6631016042780749
90000,1.4090687,0.00024898973,0.7474489795918368,11.748724489795919,0.5483566,0.7474489795918368
